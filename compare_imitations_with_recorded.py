# -*- coding: utf-8 -*-
"""compare_imitations_with_recorded.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IwCNI1xpnAd1saoi7TQgBl9WEm18uTTF
"""

# Google　Colaboratory で実行する場合はインストールする
if "google.colab" in str(get_ipython()):
    !pip install fastdtw
import librosa
import numpy as np
from fastdtw import fastdtw
from scipy.signal import correlate
from scipy.spatial.distance import euclidean
import matplotlib.pyplot as plt
from scipy.spatial.distance import cosine
from librosa.sequence import dtw


def compare_doraemon(audio_path1,audio_path2,Weights):
    #ピッチの得点
    pitch_score = compare_pitch(audio_path1, audio_path2) * Weights[0]

    # 抑揚の得点
    intonation_score = compare_intonation(audio_path1, audio_path2) * Weights[1]

    #mfccの得点
    mfcc_score = evaluate_similarity(audio_path1, audio_path2) * Weights[2]

    #リズムの得点
    rhythm_score = rhythm_compare(audio_path1,audio_path2) * Weights[3]

    #速度の得点
    speed_score = speed_compare(audio_path1,audio_path2) * Weights[4]

    #総合得点の計算
    Score = pitch_score + intonation_score + mfcc_score + rhythm_score + speed_score

    # compare_segments(audio_path1,audio_path2)

    #得点の表示
    print("Pitch Score:", pitch_score)
    print("Intonation Score:", intonation_score)
    print("MFCC Score:", mfcc_score)
    print("Rhythm Score:", rhythm_score)
    print("Speed Score:", speed_score)
    print("Total Score:", Score)


def compare_pitch(audio_path1, audio_path2):
    # 音声の基本周波数（F0）を抽出
    y1, sr1 = librosa.load(audio_path1)
    y2, sr2 = librosa.load(audio_path2)
    Max_f0 = 500
    Min_f0 = 100
    f0_1, _, _ = librosa.pyin(y1, fmin = Min_f0, fmax = Max_f0)
    f0_2, _, _ = librosa.pyin(y2, fmin = Min_f0, fmax = Max_f0)

    # NaNを除去
    f0_1 = f0_1[~np.isnan(f0_1)]
    f0_2 = f0_2[~np.isnan(f0_2)]
    # fastdtwを使用して基本周波数の動的時間伸縮距離を計算
    distance, _ = fastdtw(f0_1, f0_2)

    pitch_score = 0

    if distance >= 10000:
        pitch_score = 0
    elif distance <= 2000:
        pitch_score = 1
    else:
        pitch_score = 1.25 - distance/8000

    if distance == 0:
      return False
    else:
      return pitch_score

# 抑揚（ピッチとエネルギー変動）の類似度を計算する関数
def compare_intonation(audio_path1, audio_path2):
    # ピッチ（F0）とエネルギー（RMS）の変動を抽出
    y1, sr1 = librosa.load(audio_path1)
    y2, sr2 = librosa.load(audio_path2)

    rms_1 = librosa.feature.rms(y=y1)[0]
    rms_2 = librosa.feature.rms(y=y2)[0]

    if len(rms_2) != 0:
      rms_1 = rms_1 / np.max(rms_1)
      rms_2 = rms_2 / np.max(rms_2)
    # fastdtwを使用して抑揚の動的時間伸縮距離を計算
    distance, _ = fastdtw(rms_1, rms_2)

    intonation_score = 0

    if distance >= 30:
        intonation_score = 0
    elif distance <= 5:
        intonation_score = 1
    else:
        intonation_score = 1.2 - distance/25
    return intonation_score

def extract_mfcc(file_path, n_mfcc=13):
    # 音声ファイルを読み込み
    y, sr = librosa.load(file_path, sr=None)

    # MFCC特徴量を抽出
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)

    return mfcc

# コサイン類似度を計算する関数
def cosine_similarity(vec1, vec2):
    return 1 - cosine(vec1, vec2)

def dtw_distance(mfcc1, mfcc2):
    # DTWを計算
    D, wp = dtw(X=mfcc1, Y=mfcc2, metric='euclidean')

    # 最小コストを返す
    return D[-1, -1]

# 話者の類似性を評価する関数
def evaluate_similarity(file1, file2):
    # 2つの音声ファイルからMFCCを抽出
    mfcc1 = extract_mfcc(file1)
    mfcc2 = extract_mfcc(file2)

    # コサイン類似度を計算
    distance = dtw_distance(mfcc1, mfcc2)

    mfcc_score = 0

    if distance>= 90000:
        mfcc_score = 0
    elif distance <= 30000:
        mfcc_score = 1
    else:
        mfcc_score = 1.5 - distance/60000
    return mfcc_score

# 音声データをロードする関数
def load_audio(file_path, sr=22050):
    audio, _ = librosa.load(file_path, sr=sr)
    return audio

# ゼロクロス率を計算する関数
def compute_zero_crossing_rate(audio_path, frame_length=2048, hop_length=512):
    return librosa.feature.zero_crossing_rate(audio_path, frame_length=frame_length, hop_length=hop_length)

# テンポ（BPM）を計算する関数
def compute_tempo(audio_path, sr=22050):
    onset_env = librosa.onset.onset_strength(y=audio_path, sr=sr)
    tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)
    return tempo

# ゼロクロス率の類似度を計算する関数
def zero_crossing_rate_similarity(zcr1, zcr2):
    # ゼロクロス率の平均を比較
    zcr1_mean = np.mean(zcr1)
    zcr2_mean = np.mean(zcr2)
    return 1 - np.abs(zcr1_mean - zcr2_mean) / max(zcr1_mean, zcr2_mean)

# テンポの類似度を計算する関数
def tempo_similarity(tempo1, tempo2):
    return 1 - np.abs(tempo1 - tempo2) / max(tempo1, tempo2)


# リズムの類似度を計算
def rhythm_similarity(correlation):
    # 最大相互相関値を取る位置を特定
    max_correlation = np.max(np.abs(correlation))
    # 類似度（最大相互相関値を基準にする）
    similarity = max_correlation
    return similarity

def calculate_cross_correlation(wave1, wave2):
    # 相互相関を計算
    correlation = correlate(wave1, wave2, mode='full')
    return correlation

def rhythm_compare(audio_path1, audio_path2):
    audio1 = load_audio(audio_path1)
    audio2 = load_audio(audio_path2)
    Max_f0 = 500
    Min_f0 = 100
    f1, _, _ = librosa.pyin(audio1, fmin = Min_f0, fmax = Max_f0)
    f2, _, _ = librosa.pyin(audio2, fmin = Min_f0, fmax = Max_f0)

    # NaNを除去
    f1 = f1[~np.isnan(f1)]
    f2 = f2[~np.isnan(f2)]
    # fastdtwを使用して基本周波数の動的時間伸縮距離を計算
    _,F=fastdtw(f1,f2)
    if(len(F)==0):
      print("False")
      return "False"
     #テンポの類似性
    pa=0
    er=0
    s=0
    l=(F[len(F)-1][0]**2+F[len(F)-1][1]**2)**0.5
    for co in range(len(F)-1):
      er=((F[co+1][0]-F[co][0])+(F[co+1][1]-F[co][1]))**0.5
      pa+=er
    ry=(pa-l)/(F[len(F)-1][0]+F[len(F)-1][1]-l)

    if ry >= 0.8:
       rhythm_score = 0
    elif ry <= 0.2:
       rhythm_score = 1
    else:
       rhythm_score = 4/3 - ry/0.6
    return rhythm_score

def speed_compare(audio_path1, audio_path2):
    #喋る速さ測定
    audio1 = load_audio(audio_path1)
    audio2 = load_audio(audio_path2)

    sp=0
    if(len(audio1))>(len(audio2) - 11025):
      sp=1-(len(audio2) - 11025)/(len(audio1))
    if(len(audio2) - 11025)>=(len(audio1)):
      sp=1-(len(audio1))/(len(audio2) - 11025)

    sp_score = 0
    if sp >= 0.6:
       sp_score = 0
    elif sp <= 0.1:
       sp_score = 1
    else:
       sp_score = 1.2 - 2*sp
    return sp_score


def split_data(data, n_parts):
    """時系列データを指定した部分数に分割"""
    length = len(data)
    split_size = length // n_parts
    splits = [data[i * split_size:(i + 1) * split_size] for i in range(n_parts - 1)]
    splits.append(data[(n_parts - 1) * split_size:])  # 残りを最後に追加
    return splits

def calculate_dtw_similarity(data1, data2):
    """DTW距離を計算"""
    distance, path = fastdtw(data1, data2)
    return distance, path

def split_data_by_dtw_path(path, n_parts, data2):
    """DTWの対応パスからdata2を分割"""
    path_points = [p[1] for p in path]  # data2のインデックスを抽出
    segment_size = len(path_points) // n_parts
    split_indices = [path_points[i * segment_size] for i in range(1, n_parts)]
    split_indices = sorted(set(split_indices))  # 重複を排除してソート

    segments = []
    start_idx = 0
    for idx in split_indices:
        segments.append(data2[start_idx:idx + 1])
        start_idx = idx + 1
    segments.append(data2[start_idx:])
    return segments

def calculate_similarity(distance, max_distance):
    """距離を類似度（0から1の範囲）に変換"""
    return max(0, 1 - (distance / max_distance))

def evaluate_segments(data1, data2, n_parts):
    """時系列データを分割して各部分の類似度を計算"""
    segments1 = split_data(data2, n_parts)

    # 全体のDTWパスを計算し、それを基にdata2を分割
    _, path = calculate_dtw_similarity(data1, data2)
    segments2 = split_data_by_dtw_path(path, n_parts, data2)

    results = []
    for i, (seg1, seg2) in enumerate(zip(segments1, segments2)):
        distance, path = calculate_dtw_similarity(seg1, seg2)
        results.append((i + 1, distance, path))
    return results

def segmented_pitch_and_intonation(audio_path1, audio_path2):
    # ピッチ（F0）とエネルギー（RMS）の変動を抽出
    y1, sr1 = librosa.load(audio_path1)
    y2, sr2 = librosa.load(audio_path2)

    rms_1 = librosa.feature.rms(y=y1)[0]
    rms_2 = librosa.feature.rms(y=y2)[0]

    Max_f0 = 500
    Min_f0 = 100
    f0_1, _, _ = librosa.pyin(y1, fmin = Min_f0, fmax = Max_f0)
    f0_2, _, _ = librosa.pyin(y2, fmin = Min_f0, fmax = Max_f0)

    # NaNを除去
    f0_1 = f0_1[~np.isnan(f0_1)]
    f0_2 = f0_2[~np.isnan(f0_2)]

    # 時系列データを3つに分割して評価
    n_parts = 3

    pitch_distances = []
    intonation_distances = []

    segments1_pitch = split_data(f0_1, n_parts) # Split pitch data
    segments1_intonation = split_data(rms_1, n_parts) # Split intonation data

    _, path_pitch = calculate_dtw_similarity(f0_1, f0_2)
    _, path_intonation = calculate_dtw_similarity(rms_1, rms_2)

    segments2_pitch = split_data_by_dtw_path(path_pitch, n_parts, f0_2)
    segments2_intonation = split_data_by_dtw_path(path_intonation, n_parts, rms_2)

    for i in range(n_parts):
        distance_pitch, _ = calculate_dtw_similarity(segments1_pitch[i], segments2_pitch[i])
        distance_intonation, _ = calculate_dtw_similarity(segments1_intonation[i], segments2_intonation[i])

        pitch_distances.append(distance_pitch)
        intonation_distances.append(distance_intonation)

    pitch_similarities = [calculate_similarity(d, max(pitch_distances)) for d in pitch_distances]
    intonation_similarities = [calculate_similarity(d, max(intonation_distances)) for d in intonation_distances]


    segments = ["序盤", "中盤", "終盤"]


    # --- Generate feedback ---
    min_pitch_index = pitch_similarities.index(min(pitch_similarities))
    min_intonation_index = intonation_similarities.index(min(intonation_similarities))

    min_pitch_similarity = min(pitch_similarities)
    min_intonation_similarity = min(intonation_similarities)

    if min_pitch_similarity > 0.9 and min_intonation_similarity > 0.9:
        print("ほぼ完璧です！")
    elif min_pitch_similarity > 0.7 and min_intonation_similarity > 0.7:
        print("かなり似ています！")
    elif min_pitch_similarity > 0.5 and min_intonation_similarity > 0.5:
        print("悪くないですね。")
    elif min_pitch_similarity > 0.3 and min_intonation_similarity > 0.3:
        print("あまり似ていません。")
    else:
        print("似ていません。")
    if min_pitch_similarity < min_intonation_similarity:
        print(f" 特に {segments[min_pitch_index]} の音高に修正が必要です。")
    else:
        print(f"特に {segments[min_intonation_index]} の抑揚に修正が必要です。")

#特徴量の重み
pitch_W = 30
intonation_W = 20
mfcc_W = 30
rhythm_W = 10
speed_W = 10
Weights = [pitch_W, intonation_W, mfcc_W, rhythm_W, speed_W]

#データのパス
doraemon01 = 'doraemon01.mp3'
doraemon02 = 'doraemon02.mp3'
doraemon03 = 'doraemon03.mp3'
doraemon04 = 'doraemon04.mp3'
doraemon05 = 'doraemon05.mp3'

similar01 = 'similar01.mp3'
similar02 = 'similar02.mp3'
similar03 = 'similar03.mp3'

differ01 = 'differ01.mp3'
differ02 = 'differ02.mp3'

effect = 'effect.mp3'
obake = 'obake.mp3'

anpanman01 = 'anpanman01.wav'
anpanman02 = 'anpanman02.wav'
anpanman03 = 'anpanman03.wav'
anpanman04 = 'anpanman04.wav'
anpanman05 = 'anpanman05.wav'

anpanman_hiro_s = 'anpan_hiro.wav'
anpanman_hiro_d = 'noanpan_hiro.wav'

anpanman_rin_s = 'anpanman-rin-similar.wav'
anpanman_rin_d = 'anpanman-rin-differ.wav'

anpanman_fumiki_s = 'anpanman-fumiki-similar.wav'
anpanman_fumiki_d = 'anpanman-fumiki-differ.wav'

sazae01 = 'sazae01.wav'
sazae02 = 'sazae02.wav'
sazae03 = 'sazae03.wav'
sazae04 = 'sazae04.wav'
sazae05 = 'sazae05.wav'

sazae_hiro_s = 'sazae_hiro.wav'
sazae_hiro_d = 'nosazae_hiro.wav'

sazae_rin_s = 'sazaesan-rin-similar.wav'
sazae_rin_d = 'sazaesan-rin-differ.wav'

sazae_fumiki_s = 'sazae-fumiki-similar.wav'
sazae_fumiki_d = 'sazae-fumiki-differ.wav'


#Main

# doraemon01 と他の音声ファイルを比較
# files_to_compare = [doraemon02, doraemon03, doraemon04, doraemon05]
# for file in files_to_compare:
#     print(f"【Comparing doraemon01 with {file}】")
#     compare_doraemon(doraemon01, file, Weights)
#     print()

# anpanman01 と他の音声ファイルを比較
# files_to_compare = [anpanman02, anpanman03, anpanman04, anpanman05]
# files_to_compare = [anpanman_hiro_s, anpanman_hiro_d, anpanman_rin_s, anpanman_rin_d, anpanman_fumiki_s, anpanman_fumiki_d]

# files_to_compare = [anpanman02, anpanman03, anpanman04, anpanman05]
# for file in files_to_compare:
#     print(f"【Comparing anpanman01 with {file}】")
#     compare_doraemon(anpanman01, file, Weights)
#     print()

# files_to_compare = [sazae02, sazae03, sazae04, sazae05]
# files_to_compare = [sazae_hiro_s, sazae_hiro_d, sazae_rin_s, sazae_rin_d, sazae_fumiki_s, sazae_fumiki_d]
# for file in files_to_compare:
#     print(f"【Comparing sazae01 with {file}】")
#     compare_doraemon(sazae01, file, Weights)
#     print()

segmented_pitch_and_intonation(doraemon01, doraemon02)

